{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7da5e67b-ea21-45fc-96b7-b6f0fb616286",
   "metadata": {},
   "source": [
    "# Spaceship. Part 02.\n",
    "## Submission\n",
    "\n",
    "Here are best parameters for Random Forests so far:{'n_estimators': 1868, 'min_samples_leaf': 13, 'max_features': 'sqrt'}.\n",
    "\n",
    "Best ROC AUC cross-validation score is 0.8727799993801867."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1581182b-c847-4196-9acd-63d40e5a28ae",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": null,
    "lastExecutedAt": null,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": null,
    "outputsMetadata": {
     "0": {
      "height": 94,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC score on a train set: 0.9301234657433379\n",
      "Accuracy on a train set: 0.8419417922466352\n",
      "CPU times: total: 31.4 s\n",
      "Wall time: 32 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "data = pd.read_csv('train_prepared.csv', index_col=0)\n",
    "\n",
    "X_train = data.drop('Transported', axis =1)\n",
    "y_train = data['Transported']\n",
    "\n",
    "# Random seed for reproducibility\n",
    "SEED = 123\n",
    "\n",
    "# Import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Instantiate rf\n",
    "rf = RandomForestClassifier(random_state=SEED, n_estimators= 1868, min_samples_leaf= 13, \\\n",
    "                            max_features= 'sqrt')\n",
    "# Fit \n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "#Predict \n",
    "y_pred_train_proba = rf.predict_proba(X_train)[:, 1]\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "print(\"ROC AUC score on a train set: {}\".format(roc_auc_score(y_train, y_pred_train_proba)))\n",
    "print(\"Accuracy on a train set: {}\".format(rf.score(X_train, y_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0de966",
   "metadata": {},
   "source": [
    "Now, let's create our predictions on a test set and submit them to a Spaceship competition to receive a test accuracy score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5add695c-85ac-4661-b484-d36546f06848",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.read_csv('X_test_prepared.csv', index_col=0)\n",
    "test_Ids = pd.read_csv('test_Ids.csv', index_col=0).reset_index(drop=True)\n",
    "\n",
    "y_pred_test = rf.predict(X_test)\n",
    "\n",
    "# Convert to 'True/False'\n",
    "\n",
    "y_pred_test = [\"True\" if i == 1 else \"False\" for i in y_pred_test]\n",
    "\n",
    "y_pred_test = pd.DataFrame(y_pred_test, columns=['Transported'])\n",
    "\n",
    "submission = pd.concat([test_Ids, y_pred_test], axis=1)\n",
    "\n",
    "submission.to_csv('03_submission.csv', index=False)\n",
    "                           \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c440ec",
   "metadata": {},
   "source": [
    "Accuracy of our first submission is 0.79003. Let's examine how far we are from the best results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04dae9be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We labeled correctly 3379.0 passengers (one sumbission).\n",
      "AmberLi456 (4th place) labeled correctly 3531.0 passengers (30 sumbissions).\n"
     ]
    }
   ],
   "source": [
    "print('We labeled correctly {} passengers (one sumbission).'.format(round(len(X_test) * 0.79003, 1)))\n",
    "print('AmberLi456 (4th place) labeled correctly {} passengers (30 sumbissions).'.format(round(len(X_test) * 0.82557, 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b991915d",
   "metadata": {},
   "source": [
    "There is a room for improvement. We can do it in several ways:\n",
    "\n",
    "-) Try more parameters for Random Forests\n",
    "\n",
    "-) Try different classifiers\n",
    "\n",
    "-) Try greater number of PCA components\n",
    "\n",
    "-) Explore passengers that we are wrongly predicting on a train set and try to find patterns.\n",
    "\n",
    "Since our train accuracy is high, it's unlikely that our model has high bias. Most likely, we are dealing with overfitting. Therefore, two first ideas are preferable:\n",
    "\n",
    "-) Try more parameters for Random Forests\n",
    "\n",
    "-) Try different classifiers\n",
    "\n",
    "## Try more parameters for Random Forests\n",
    "\n",
    "I'll expand the parameter grid in ['03_RF.py'](03_RF.py).\n",
    "\n",
    "Some findings for different parameters of Random Forests (parameter set and cross-validation ROC AUC score):\n",
    "\n",
    "{'n_estimators': 1868, 'min_samples_leaf': 13, 'max_features': 'sqrt'} 0.8727799993801867\n",
    "\n",
    "{n_estimators= 100, 'criterion': 'entropy', 'max_depth': 12, 'min_samples_leaf': 8} 0.8740706564503291\n",
    "\n",
    "{'criterion': 'log_loss', 'max_depth': 15, 'min_samples_leaf': 5, 'min_impurity_decrease': 0.0006830666561997347}\n",
    "0.8755529319815356\n",
    "\n",
    "{'max_depth': 16, 'max_features': 0.9, 'max_leaf_nodes': 141, 'min_impurity_decrease': 9.376419510025081e-06, 'min_samples_leaf': 2, 'warm_start': False, 'max_samples': 0.8945061534250961}\n",
    "0.8764434269931722\n",
    "\n",
    "{'max_depth': 17, 'max_features': 0.7, 'max_leaf_nodes': 123, 'min_impurity_decrease': 0.00020380822483963789, 'min_samples_leaf': 2, 'max_samples': 0.9999360987512214}\n",
    "0.8770831641318569\n",
    "\n",
    "Increasing number of estimators increases ROC AUC cross-validation score, but high number is computationally expensive. So, in most of iterations I used n_estimators = 100 and I didn't use computationally expensive criterion 'entropy'. Let's try some our  best parameters with different criterions and different numbers of estimators.\n",
    "\n",
    "'Entropy' criterion didn't show any improvements. however, we've found a better number of estimators:\n",
    "\n",
    "{'n_estimators': 516}\n",
    "0.8773454605099553\n",
    "\n",
    "Now, let's train our model with the best parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ee56641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC score on a train set: 0.9240332336918978\n",
      "Accuracy on a train set: 0.8380305993327966\n",
      "CPU times: total: 30.1 s\n",
      "Wall time: 3.36 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "rf = RandomForestClassifier(random_state=SEED, \\\n",
    "                               n_estimators= 516, \\\n",
    "                               criterion= 'log_loss', \\\n",
    "                               max_depth= 17, \\\n",
    "                               max_features=0.7, \\\n",
    "                               max_leaf_nodes=123,\\\n",
    "                               min_impurity_decrease= 0.00020380822483963789, \\\n",
    "                               min_samples_leaf= 2, \\\n",
    "                               max_samples= 0.9999360987512214, \\\n",
    "                               n_jobs=-1\n",
    "                               )\n",
    "\n",
    "\n",
    "# Fit \n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "#Predict \n",
    "y_pred_train_proba = rf.predict_proba(X_train)[:, 1]\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "print(\"ROC AUC score on a train set: {}\".format(roc_auc_score(y_train, y_pred_train_proba)))\n",
    "print(\"Accuracy on a train set: {}\".format(rf.score(X_train, y_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3967ea",
   "metadata": {},
   "source": [
    "Let's submit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b703b03f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of passengers predicted differently in the second sumbission:\n",
      "128\n"
     ]
    }
   ],
   "source": [
    "y_pred_test_2 = rf.predict(X_test)\n",
    "\n",
    "# Convert to 'True/False'\n",
    "y_pred_test_2 = [\"True\" if i == 1 else \"False\" for i in y_pred_test_2]\n",
    "\n",
    "y_pred_test_2 = pd.DataFrame(y_pred_test_2, columns=['Transported'])\n",
    "\n",
    "\n",
    "print('Number of passengers predicted differently in the second sumbission:')\n",
    "print(len(y_pred_test[y_pred_test['Transported'] != y_pred_test_2['Transported']]))\n",
    "\n",
    "\n",
    "submission_2 = pd.concat([test_Ids, y_pred_test_2], axis=1)\n",
    "\n",
    "submission_2.to_csv('03_submission_2.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2520eaf1",
   "metadata": {},
   "source": [
    "Accuracy of our second submission is higher, 0.79611. Let's examine how far we are now from the best results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab726a28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the first submission, we labeled correctly 3379.0 passengers.\n",
      "In the second submission, we labeled correctly 3405.0 passengers.\n",
      "AmberLi456 (4th place) labeled correctly 3531.0 passengers (30 sumbissions).\n"
     ]
    }
   ],
   "source": [
    "print('In the first submission, we labeled correctly {} passengers.'.format(round(len(X_test) * 0.79003, 1)))\n",
    "print('In the second submission, we labeled correctly {} passengers.'.format(round(len(X_test) * 0.79611, 1)))\n",
    "print('AmberLi456 (4th place) labeled correctly {} passengers (30 sumbissions).'.format(round(len(X_test) * 0.82557, 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ec4015",
   "metadata": {},
   "source": [
    "The next most promising idea is to try different models and maybe combine different models with high ROC AUC score into an ensemble.\n",
    "\n",
    "I tried different models (see files 03_[model_name].py) and Random Forests still show the best cross-validation score. \n",
    "\n",
    "I could try model ensembling, but I will try to start over with tweaking some steps and testing every step with our best model.\n",
    "\n",
    "Let's continue in ['04_startover.ipynb'](04_startover.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1282739b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Welcome to DataCamp Workspaces.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
